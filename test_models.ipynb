{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product as iter_product\n",
    "\n",
    "import src, src.debias, src.models, src.ranking, src.datasets, src.data_utils\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    use_device_id = int(input(f\"Choose cuda index, from [0-{torch.cuda.device_count()-1}]: \").strip())\n",
    "else: use_device_id = 0\n",
    "use_device = \"cuda:\"+str(use_device_id) if torch.cuda.is_available() else \"cpu\"\n",
    "if not torch.cuda.is_available():\n",
    "    input(\"CUDA isn't available, so using cpu. Please press any key to confirm this isn't an error: \\n\")\n",
    "print(\"Using device\", use_device)\n",
    "torch.cuda.set_device(use_device_id)\n",
    "\n",
    "with open(src.PATHS.TRAINED_MODELS.METADATA, mode=\"r\") as _runs_metafile:\n",
    "    runs_metadata = json.load(_runs_metafile)\n",
    "\n",
    "with open(src.PATHS.TRAINED_MODELS.TEST_PROMPTS, mode=\"r\") as _test_promptsfile:\n",
    "    test_prompts_data = json.load(_test_promptsfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for module in [src, src.debias, src.models, src.ranking,  src.datasets, src.data_utils]:\n",
    "    reload(module)\n",
    "\n",
    "eval_dss = {\"gender\": [(\"FairFace\", \"val\"), (\"UTKFace\", \"val\"), (\"COCOGender\", \"val\"), ],#(\"CelebA\", \"val\")],  # has 200k images so takes looong to compute and we don't focus on it anyway\n",
    "            \"race\": [(\"FairFace\", \"val\"), (\"UTKFace\", \"val\")]}\n",
    "clip_arch = \"openai/CLIP/ViT-B/16\"\n",
    "\n",
    "evaluations = [\"maxskew\", \"ndkl\", \"clip_audit\"]\n",
    "perf_evaluations = [\"cifar10\", \"flickr1k\", \"cifar100\"] # flickr1k, cifar100, cifar10\n",
    "all_experiment_results = pd.DataFrame()\n",
    "clip_audit_results = pd.DataFrame()\n",
    "batch_sz = 256\n",
    "\n",
    "try:\n",
    "    with torch.cuda.device(use_device_id):\n",
    "        for run_id, run_metadata in list(runs_metadata.items()):\n",
    "            experiment_results = pd.DataFrame()\n",
    "            if int(run_id) != 91:\n",
    "                continue\n",
    "            print(run_id, run_metadata)\n",
    "            n_debias_tokens = 2 if int(run_id) < 100 else 0\n",
    "            model_save_name = f\"best_ndkl_oai-clip-vit-b-16_neptune_run_OXVLB-{run_id}_model_e{run_metadata['epoch']}_step_{run_metadata['step']}.pt\"\n",
    "\n",
    "            model, preprocess, tokenizer, model_alias = src.models.DebiasCLIP.from_cfg(src.Dotdict({\n",
    "                \"CLIP_ARCH\": clip_arch, \"DEVICE\": use_device, \"num_debias_tokens\": n_debias_tokens\n",
    "            }))\n",
    "            model_alias = model_save_name\n",
    "            model.load_state_dict(torch.load(os.path.join(src.PATHS.TRAINED_MODELS.BASE, model_save_name), map_location=use_device), strict=True)\n",
    "            model = model.eval().to(use_device)\n",
    "            debias_class = run_metadata[\"debias_class\"]\n",
    "\n",
    "            test_prompts = test_prompts_data[debias_class]\n",
    "            test_prompts_df = pd.DataFrame({\"prompt\": test_prompts})\n",
    "            test_prompts_df[\"group\"] = debias_class\n",
    "\n",
    "            if \"clip_audit\" in evaluations:\n",
    "                ca_prompts = test_prompts_data[\"clip_audit\"]\n",
    "                ca_ds = src.datasets.FairFace(iat_type=\"race\", lazy=True, _n_samples=None, transforms=preprocess, mode=\"val\")\n",
    "                ca_dl = DataLoader(ca_ds, batch_size=batch_sz, shuffle=False, num_workers=8) # Shuffling ISN'T(!) reflected in the cache\n",
    "                ca_res = src.ranking.do_clip_audit(ca_dl, ca_prompts, model, model_save_name, tokenizer, preprocess, use_device, use_templates=True)\n",
    "                for k, v in {\"model_name\": model_save_name, \"dataset\": \"FairFaceVal\",\n",
    "                            \"evaluation\": \"clip_audit\"}.items():\n",
    "                    ca_res[k] = v\n",
    "                clip_audit_results = clip_audit_results.append(ca_res, ignore_index=True)\n",
    "\n",
    "\n",
    "            for perf_eval in perf_evaluations:\n",
    "                perf_res = {\"model_name\": model_save_name, \"dataset\": perf_eval,\n",
    "                            \"evaluation\": perf_eval, \"mean\": src.debias.run_perf_eval(perf_eval, model, tokenizer, preprocess, use_device)}\n",
    "                experiment_results = experiment_results.append(pd.DataFrame([perf_res]), ignore_index=True)\n",
    "\n",
    "            n_imgs = None # First run populates cache, thus run with None first, later runs can reduce number\n",
    "            for dset_name, dset_mode in eval_dss[debias_class]:\n",
    "                ds = getattr(src.datasets, dset_name)(lazy=True, _n_samples=n_imgs, transforms=preprocess, mode=dset_mode)\n",
    "                dl = DataLoader(ds, batch_size=batch_sz, shuffle=False, num_workers=8) # Shuffling ISN'T(!) reflected in the cache\n",
    "\n",
    "                for evaluation in evaluations:\n",
    "                    if evaluation == \"clip_audit\": continue\n",
    "                    model.eval()\n",
    "                    _res = src.debias.run_bias_eval(evaluation, test_prompts_df, model, model_save_name, tokenizer, dl, use_device, cache_suffix=\"\")\n",
    "                    _res = src.debias.mean_of_bias_eval(_res, evaluation, \"dem_par\")\n",
    "                    res = {}\n",
    "                    for key, val in _res.items():\n",
    "                        for rename in [\"mean_\", \"std_\"]:\n",
    "                            if key.startswith(rename):\n",
    "                                res[rename[:-1]] = val\n",
    "                                break\n",
    "                        else:\n",
    "                            res[key] = val\n",
    "                    res[\"model_name\"] = model_save_name\n",
    "                    res[\"dataset\"] = dset_name+dset_mode.capitalize()\n",
    "                    res[\"evaluation\"] = evaluation\n",
    "                    experiment_results = experiment_results.append(pd.DataFrame([res]), ignore_index=True)\n",
    "\n",
    "            experiment_results[\"debias_class\"] = debias_class\n",
    "            experiment_results[\"train_ds\"] = run_metadata[\"train_ds\"]\n",
    "\n",
    "            all_experiment_results = all_experiment_results.append(experiment_results)\n",
    "            del model, preprocess, tokenizer\n",
    "finally:\n",
    "    result_name = f\"exp_test_bias_results.csv\"\n",
    "    ca_result_name = f\"exp_test_clip_audit_results.csv\"\n",
    "    all_experiment_results.to_csv(os.path.join(src.PATHS.PLOTS.BASE, result_name))\n",
    "    clip_audit_results.to_csv(os.path.join(src.PATHS.PLOTS.BASE, ca_result_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%i\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "display(clip_audit_results)\n",
    "display(all_experiment_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
