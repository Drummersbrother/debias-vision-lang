{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import json\n",
    "import os, sys\n",
    "from src import PATHS\n",
    "sys.path.insert(0, os.path.join(PATHS.BASE, \"wip\", \"hugo\", \"src\"))\n",
    "import parse_config\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product as iter_product\n",
    "\n",
    "import src, src.debias, src.models, src.ranking, src.datasets, src.data_utils\n",
    "from src.models import model_loader\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    use_device_id = int(input(f\"Choose cuda index, from [0-{torch.cuda.device_count()-1}]: \").strip())\n",
    "else: use_device_id = 0\n",
    "use_device = \"cuda:\"+str(use_device_id) if torch.cuda.is_available() else \"cpu\"\n",
    "if not torch.cuda.is_available():\n",
    "    input(\"CUDA isn't available, so using cpu. Please press any key to confirm this isn't an error: \\n\")\n",
    "print(\"Using device\", use_device)\n",
    "torch.cuda.set_device(use_device_id)\n",
    "\n",
    "with open(src.PATHS.TRAINED_MODELS.TEST_PROMPTS, mode=\"r\") as _test_promptsfile:\n",
    "    test_prompts_data = json.load(_test_promptsfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "debias_class = \"gender\"\n",
    "\n",
    "experiment_results = pd.DataFrame()\n",
    "test_prompts = test_prompts_data[debias_class]\n",
    "test_prompts_df = pd.DataFrame({\"prompt\": test_prompts})\n",
    "test_prompts_df[\"group\"] = debias_class\n",
    "\n",
    "test_prompts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "eval_dss = {\"gender\": [(\"FairFace\", \"val\"), (\"UTKFace\", \"val\")], \"race\": [(\"FairFace\", \"val\"), (\"UTKFace\", \"val\")]}\n",
    "evaluations = [\"maxskew\", \"ndkl\", \"clip_audit\"]\n",
    "perf_evaluations = [\"cifar10\", \"flickr1k\", \"cifar100\"] # flickr1k, cifar100, cifar10\n",
    "all_experiment_results = pd.DataFrame()\n",
    "clip_audit_results = pd.DataFrame()\n",
    "batch_sz = 256\n",
    "\n",
    "try:\n",
    "    with torch.cuda.device(use_device_id):\n",
    "        for model_name in src.models.VALID_MODELS:\n",
    "            print(model_name)\n",
    "\n",
    "            model, preprocess, tokenizer, model_alias = model_loader(model_name, device=use_device, jit=True)\n",
    "\n",
    "            if model_name.startswith(\"m-bain/\"):\n",
    "                model.logit_scale = torch.tensor(-1, dtype=torch.float32, device=use_device)\n",
    "\n",
    "            if \"clip_audit\" in evaluations:\n",
    "                ca_prompts = test_prompts_data[\"clip_audit\"]\n",
    "                ca_ds = src.datasets.FairFace(iat_type=\"race\", lazy=True, _n_samples=None, transforms=preprocess, mode=\"val\")\n",
    "                ca_dl = DataLoader(ca_ds, batch_size=batch_sz, shuffle=False, num_workers=8) # Shuffling ISN'T(!) reflected in the cache\n",
    "                ca_res = src.ranking.do_clip_audit(ca_dl, ca_prompts, model, model_alias, tokenizer, preprocess, use_device, use_templates=True)\n",
    "                for k, v in {\"model_name\": model_alias, \"dataset\": \"FairFaceVal\",\n",
    "                             \"evaluation\": \"clip_audit\"}.items():\n",
    "                    ca_res[k] = v\n",
    "                clip_audit_results = clip_audit_results.append(ca_res, ignore_index=True)\n",
    "\n",
    "                print(\"Done with clip audit\")\n",
    "\n",
    "\n",
    "            for debias_class in {\"gender\", \"race\"}:\n",
    "                experiment_results = pd.DataFrame()\n",
    "                test_prompts = test_prompts_data[debias_class]\n",
    "                test_prompts_df = pd.DataFrame({\"prompt\": test_prompts})\n",
    "                test_prompts_df[\"group\"] = debias_class\n",
    "\n",
    "                for perf_eval in perf_evaluations:\n",
    "                    perf_res = {\"model_name\": model_alias, \"dataset\": perf_eval,\n",
    "                                \"evaluation\": perf_eval, \"debias_class\": debias_class, \"mean\": src.debias.run_perf_eval(perf_eval, model, tokenizer, preprocess, use_device)}\n",
    "                    experiment_results = experiment_results.append(pd.DataFrame([perf_res]), ignore_index=True)\n",
    "\n",
    "                n_imgs = None # First run populates cache, thus run with None first, later runs can reduce number\n",
    "                for dset_name, dset_mode in eval_dss[debias_class]:\n",
    "                    ds = getattr(src.datasets, dset_name)(iat_type=debias_class, lazy=True, _n_samples=n_imgs, transforms=preprocess, mode=dset_mode)\n",
    "                    dl = DataLoader(ds, batch_size=batch_sz, shuffle=False, num_workers=8) # Shuffling ISN'T(!) reflected in the cache\n",
    "\n",
    "                    for evaluation in evaluations:\n",
    "                        if evaluation == \"clip_audit\": continue\n",
    "                        model.eval()\n",
    "                        _res = src.debias.run_bias_eval(evaluation, test_prompts_df, model, model_alias, tokenizer, dl, use_device, cache_suffix=\"\")\n",
    "                        _res = src.debias.mean_of_bias_eval(_res, evaluation, \"dem_par\")\n",
    "                        res = {}\n",
    "                        for key, val in _res.items():\n",
    "                            for rename in [\"mean_\", \"std_\"]:\n",
    "                                if key.startswith(rename):\n",
    "                                    res[rename[:-1]] = val\n",
    "                                    break\n",
    "                            else:\n",
    "                                res[key] = val\n",
    "                        res[\"model_name\"] = model_alias\n",
    "                        res[\"dataset\"] = dset_name+dset_mode.capitalize()\n",
    "                        res[\"evaluation\"] = evaluation\n",
    "                        experiment_results = experiment_results.append(pd.DataFrame([res]), ignore_index=True)\n",
    "\n",
    "                experiment_results[\"debias_class\"] = debias_class\n",
    "\n",
    "                all_experiment_results = all_experiment_results.append(experiment_results)\n",
    "        #del model, preprocess, tokenizer\n",
    "finally:\n",
    "    result_name = f\"untrained_test_bias_results.csv\"\n",
    "    ca_result_name = f\"untrained_test_clip_audit_results.csv\"\n",
    "    all_experiment_results.to_csv(os.path.join(src.PATHS.PLOTS.BASE, result_name))\n",
    "    clip_audit_results.to_csv(os.path.join(src.PATHS.PLOTS.BASE, ca_result_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "display(clip_audit_results)\n",
    "display(all_experiment_results)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f74b83fbac03ad89a07102ed67580a2c0f75e7387e9775635bfd4562de1558a5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('bias-vision-language-yfXTBDV4': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
