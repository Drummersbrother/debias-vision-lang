{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import src.data_utils\n",
    "\n",
    "use_device = torch.device(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clip_arch = \"openai/CLIP/ViT-B/16\"\n",
    "n_debias_tokens = 2\n",
    "debias_class = \"gender\" # Gender or race\n",
    "\n",
    "model, preprocess, tokenizer, model_alias = src.models.DebiasCLIP.from_cfg(src.Dotdict({\n",
    "    \"CLIP_ARCH\": clip_arch, \"DEVICE\": use_device, \"num_debias_tokens\": n_debias_tokens\n",
    "}))\n",
    "model.debias_tokens.load_state_dict(\n",
    "    torch.load(os.path.join(src.PATHS.TRAINED_MODELS.EMBEDDINGS, f\"{debias_class}_{n_debias_tokens}.pth\"), map_location=use_device),\n",
    "    strict=True)\n",
    "model = model.eval().to(use_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "DebiasCLIP(\n  (image_encoder): VisionTransformer(\n    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (transformer): Transformer(\n      (resblocks): Sequential(\n        (0): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (1): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (2): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (3): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (4): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (5): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (6): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (7): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (8): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (9): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (10): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (11): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n          )\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (clip): CLIP(\n    (visual): VisionTransformer(\n      (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (transformer): Transformer(\n        (resblocks): Sequential(\n          (0): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (1): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (2): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (3): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (4): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (5): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (6): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (7): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (8): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (9): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (10): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (11): ResidualAttentionBlock(\n            (attn): MultiheadAttention(\n              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n            )\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): Sequential(\n              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n              (gelu): QuickGELU()\n              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (transformer): Transformer(\n      (resblocks): Sequential(\n        (0): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (1): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (2): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (3): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (4): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (5): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (6): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (7): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (8): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (9): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (10): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n        (11): ResidualAttentionBlock(\n          (attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (mlp): Sequential(\n            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n            (gelu): QuickGELU()\n            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          )\n          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (token_embedding): Embedding(49408, 512)\n    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  )\n  (debias_tokens): Embedding(2, 512)\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}