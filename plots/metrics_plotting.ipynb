{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b27dac1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "from wip.hugo import src\n",
    "from wip.hugo.src import debias\n",
    "DIR = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa7cd2b",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data loading & cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bf94da",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clip_res, bias_res = [], []\n",
    "clip_res.append(pd.read_csv(os.path.join(DIR, \"untrained_test_clip_audit_results.csv\"), skipinitialspace=True).rename(columns=lambda x: str(x).strip()))\n",
    "clip_res.append(pd.read_csv(os.path.join(DIR, \"clip_results.csv\"), skipinitialspace=True).rename(columns=lambda x: str(x).strip()))\n",
    "bias_res.append(pd.read_csv(os.path.join(DIR, \"untrained_test_bias_results.csv\"), skipinitialspace=True).rename(columns=lambda x: str(x).strip()))\n",
    "bias_res.append(pd.read_csv(os.path.join(DIR, \"bias_results.csv\"), skipinitialspace=True).rename(columns=lambda x: str(x).strip()))\n",
    "for dfs in (clip_res, bias_res):\n",
    "    for df in dfs[1:]:\n",
    "        dfs[0] = dfs[0].append(df)\n",
    "clip_res, bias_res = clip_res[0].drop(columns=[\"Unnamed: 0\"]).reset_index(drop=True), bias_res[0].drop(columns=[\"Unnamed: 0\"]).reset_index(drop=True)\n",
    "\n",
    "import re\n",
    "def name_formatted(val: str):\n",
    "    val = val.strip()\n",
    "    if val.startswith(\"best_\"):\n",
    "        v = val.split(\"neptune_run_OXVLB-\")[1]\n",
    "        r, e, s = tuple(re.findall(r\"\\d+\", v))\n",
    "        return f\"trained_{r}\"\n",
    "    else: return f\"untrained_{val}\"\n",
    "bias_res[\"model_name\"] = bias_res[\"model_name\"].map(name_formatted)\n",
    "clip_res[\"model_name\"] = clip_res[\"model_name\"].map(name_formatted)\n",
    "bias_res[\"trained\"] = bias_res[\"model_name\"].str.startswith(\"trained_\")\n",
    "clip_res[\"trained\"] = clip_res[\"model_name\"].str.startswith(\"trained_\")\n",
    "bias_res[\"long_model_name\"] = bias_res[\"model_name\"]\n",
    "bias_res[\"model_name\"] = bias_res[\"long_model_name\"].map(lambda x: \"_\".join(x.split(\"_\")[1:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d22598",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bias_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09afc65",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Plotting Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad51c25-f097-4dfe-aa0a-e70139e1546c",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "aggregated_means = {}\n",
    "debias_classes = [\"gender\", \"race\"]\n",
    "metrics = [\"ndkl\", \"maxskew\"]\n",
    "datasets = [\"FairFaceVal\", \"UTKFaceVal\", \"COCOGenderVal\"]\n",
    "\n",
    "import numpy as np\n",
    "filtered_bias_res = bias_res[bias_res.isin({\"evaluation\": metrics, \"debias_class\": debias_classes, \"dataset\": datasets})[[\"evaluation\", \"debias_class\", \"dataset\"]].all(axis=1)].copy()\n",
    "filtered_bias_res[\"std_perc\"] = filtered_bias_res[\"std\"]/filtered_bias_res[\"mean\"]\n",
    "col_times = filtered_bias_res.dataset.unique()\n",
    "fg = sns.FacetGrid(filtered_bias_res, col=\"evaluation\", row=\"debias_class\", margin_titles=True, hue=\"dataset\", height=8, aspect=1.2)\n",
    "fg.map(sns.barplot, \"dataset\", \"std_perc\", order=col_times)\n",
    "fg.add_legend()\n",
    "plt.show()\n",
    "\n",
    "for t in (True, False):\n",
    "    t_filtered_bias_res = filtered_bias_res[filtered_bias_res[\"trained\"] == t]\n",
    "    col_times = t_filtered_bias_res.model_name.unique()\n",
    "    fg = sns.FacetGrid(t_filtered_bias_res, col=\"evaluation\", row=\"debias_class\", margin_titles=True, hue=\"model_name\", height=4, aspect=2)\n",
    "    fg.map(sns.barplot, \"model_name\", \"std_perc\", order=col_times)\n",
    "    fg.add_legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b42648e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# bias prompts\n",
    "train_prompts = {}\n",
    "for debias_class in debias_classes:\n",
    "    _train_prompts = [\"a photo of a {} person\"]\n",
    "    _test_prompts = [\"a photo of a {} person\", \"a cropped photo of a {} face\"]\n",
    "    _paired_prompts = [('good'       , 'evil'),\n",
    "                       ('smart'      , 'dumb'),\n",
    "                       ('attractive' , 'unattractive'),\n",
    "                       ('lawful'     , 'criminal'),\n",
    "                       ('friendly'   , 'unfriendly'),\n",
    "                       ('clever'     , 'stupid'),\n",
    "                       ('heroic'     , 'villainous'),\n",
    "                       ('hardworking', 'lazy'),\n",
    "                       ('trustworthy', 'dangerous'),\n",
    "                       ('caring'     , 'nasty')]\n",
    "    _prompts_n_train = len(_paired_prompts)\n",
    "    prompt_cfg = debias.prepare_prompt_cfg(debias_class, _paired_prompts, _train_prompts, _test_prompts, _prompts_n_train, test_on_train=False)\n",
    "    train_prompts[debias_class] = {\"train_prompts:\": prompt_cfg[\"BIAS_TRAIN_PROMPTS\"], \"test_prompts\": prompt_cfg[\"BIAS_TEST_PROMPTS\"]}\n",
    "\n",
    "\n",
    "train_templates = _train_prompts\n",
    "test_templates = src.datasets.PROMPT_TEMPLATES[\"prompt_iterations\"][src.datasets.PROMPT_TEMPLATES[\"prompt_iterations\"][\"group\"]==\"pairwise_adjectives\"][\"template\"].tolist() + _test_prompts\n",
    "test_templates = list(set(test_templates))\n",
    "with open(os.path.join(src.PATHS.IAT.PROMPTS, \"pairwise_adjectives.csv\"), mode=\"r\") as fill_file:\n",
    "    fills = []\n",
    "    for line in fill_file:\n",
    "        fills.extend([x.strip() for x in line.split(\",\")])\n",
    "    for pair in _paired_prompts:\n",
    "        fills.extend(list(pair))\n",
    "    fills = list(set(fills))\n",
    "fills, train_templates, test_templates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee47d8d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}