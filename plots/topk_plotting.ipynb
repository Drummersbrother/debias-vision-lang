{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from importlib import reload\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm, trange\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product as iter_product\n",
    "\n",
    "import src, src.debias, src.models, src.ranking, src.datasets, src.data_utils\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    use_device_id = int(input(f\"Choose cuda index, from [0-{torch.cuda.device_count() - 1}]: \").strip())\n",
    "else:\n",
    "    use_device_id = 0\n",
    "use_device = \"cuda:\" + str(use_device_id) if torch.cuda.is_available() else \"cpu\"\n",
    "if not torch.cuda.is_available():\n",
    "    input(\"CUDA isn't available, so using cpu. Please press any key to confirm this isn't an error: \\n\")\n",
    "print(\"Using device\", use_device)\n",
    "torch.cuda.set_device(use_device_id)\n",
    "\n",
    "with open(src.PATHS.TRAINED_MODELS.METADATA, mode=\"r\") as _runs_metafile:\n",
    "    runs_metadata = json.load(_runs_metafile)\n",
    "\n",
    "clip_arch = \"openai/CLIP/ViT-B/16\"\n",
    "\n",
    "run_id = \"91\"\n",
    "run_metadata = runs_metadata[run_id]\n",
    "model_save_name = f\"best_ndkl_oai-clip-vit-b-16_neptune_run_OXVLB-91_model_e{run_metadata['epoch']}_step_{run_metadata['step']}.pt\"\n",
    "n_debias_tokens = 2\n",
    "\n",
    "models = []\n",
    "model_aliases = []\n",
    "with torch.cuda.device(use_device_id):\n",
    "    for k in (True, False):\n",
    "        model, preprocess, tokenizer, model_alias = src.models.DebiasCLIP.from_cfg(src.Dotdict({\n",
    "            \"CLIP_ARCH\": clip_arch, \"DEVICE\": use_device, \"num_debias_tokens\": n_debias_tokens if k else 0\n",
    "        }))\n",
    "        if k:\n",
    "            model.load_state_dict(\n",
    "                torch.load(os.path.join(src.PATHS.TRAINED_MODELS.BASE, model_save_name), map_location=use_device),\n",
    "                strict=True)\n",
    "        models.append(model.eval().to(use_device))\n",
    "        model_aliases.append(model_alias)\n",
    "\n",
    "tmodel, umodel = models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%i\n"
    }
   },
   "outputs": [],
   "source": [
    "from_topk = 10\n",
    "return_k = 10\n",
    "\n",
    "prompts = [\"a photo of a smart person\"] # example prompt\n",
    "dataset = \"FairFace\"\n",
    "\n",
    "ds = getattr(src.datasets, dataset)(lazy=True, _n_samples=None, transforms=preprocess, mode=\"val\")\n",
    "dl = DataLoader(ds, batch_size=256, shuffle=False, num_workers=8)\n",
    "\n",
    "with torch.no_grad():\n",
    "    prompt_embeds = torch.stack([model.encode_text(tokenizer(prompts).to(use_device)) for model in models])\n",
    "    prompt_embeds /= torch.norm(prompt_embeds, dim=-1, keepdim=True)\n",
    "\n",
    "    img_embeds = []\n",
    "    for model, trained in zip(models, (True, False)):\n",
    "        img_embeds.append(src.datasets.compute_img_embeddings(ds, model, (f\"train_run_{run_id}_\" if trained else \"\")+model_alias, device=use_device))\n",
    "    img_embeds = torch.stack(img_embeds)\n",
    "    img_embeds /= torch.norm(img_embeds, dim=-1, keepdim=True)\n",
    "\n",
    "import random\n",
    "rng = random.Random()\n",
    "res = {}\n",
    "for i, prompt in enumerate(prompts):\n",
    "    res[prompt] = {}\n",
    "    for j, trained in enumerate((\"trained\", \"untrained\")):\n",
    "        prompt_embed = prompt_embeds[j][i].to(use_device)\n",
    "        img_embed = img_embeds[j].to(use_device)\n",
    "        top_indices = (prompt_embed @ img_embed.T).topk(from_topk).indices.cpu().tolist()\n",
    "        print(prompt, trained, (prompt_embed@img_embed.T).min())\n",
    "        res[prompt][trained] = rng.sample(top_indices, return_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "for prompt, pvals in res.items():\n",
    "    print(prompt)\n",
    "    for trained, tvals in pvals.items():\n",
    "        print(\"\\t\", prompt)\n",
    "        n_female = 0\n",
    "        for inx in tvals:\n",
    "            sample = ds[inx]\n",
    "            image = Image.open(ds._img_fnames[inx])\n",
    "            print(sample.gender)\n",
    "            if sample.gender == \"Female\": n_female += 1\n",
    "            display(image)\n",
    "        print(f\"\\t\\t% deviation from parity: {abs(0.5-(n_female/return_k)):.1%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}